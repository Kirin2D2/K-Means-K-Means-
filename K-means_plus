#initialization method for Kmeans++
def plus_init(X,k):
    '''
    Inputs:
    X is a 2-D Numpy array of shape (n, d), each row is a data point
    k is the number of clusters
    Outputs:
    C a 2-D Numpy array of shape (k, d) of points at which to initialise centroids for k-means++
    '''
    n, d = X.shape[0], X.shape[1]
    C = np.zeros((k, d), dtype=float)
    remain_index = list(range(n))

    # random init
    zero_init = np.random.randint(n)
    C[0] = X[zero_init]
    remain_index.remove(zero_init)
    for idx in range(1, k):
        dx2 = compute_sq_dists(X[remain_index], C[:idx])
        dx2 = np.min(dx2, axis=-1)
        prob_dx = dx2 / np.sum(dx2)
        new_idx = np.random.choice(remain_index, p=prob_dx)
        C[idx] = X[new_idx]
        remain_index.remove(new_idx)
    return C

def k_means_plus(X, k, max_iters=50, eps=1e-5):
    '''
    Perform k-means++ clusting on X using k clusters, running for max_iters loops,
    and stop early if loss decreases by a proportion less than eps.
    Early stopping does not occur before epoch 2.

    Output (W, loss): the final centers W and loss is a list containing loss values at each iteration
    '''

    W = plus_init(X, k) #initialization
    previous_loss, assoc = associate_centroids(X, W)
    loss = [previous_loss]

    for i in range(max_iters - 1):
      W = update_centroids(X, k, assoc)
      current_loss, assoc = associate_centroids(X, W)
      loss.append(current_loss)
      if (previous_loss - current_loss) / current_loss < eps:
        break
      previous_loss = current_loss

    return W, loss
